% Modulo 13 — Automazione e Web Scraping
\chapter{Automazione e Web Scraping}

\begin{tcolorbox}[title=Obiettivi del capitolo]
Dopo questo capitolo saprai:
\begin{itemize}
  \item Automatizzare task con subprocess e schedule
  \item Effettuare richieste HTTP con requests
  \item Parsare HTML con BeautifulSoup
  \item Consumare API REST
  \item Scrapare siti dinamici con Selenium
  \item Rispettare etica e robots.txt
  \item Implementare rate limiting e retry logic
  \item Salvare dati (CSV, JSON, database)
  \item Schedulare script automatici
\end{itemize}
\end{tcolorbox}

\section{Automazione con subprocess}

\subsection{Eseguire comandi sistema}

\begin{lstlisting}
import subprocess

# Run command e cattura output
result = subprocess.run(
    ["ls", "-la"],
    capture_output=True,
    text=True
)

print(result.stdout)
print(f"Exit code: {result.returncode}")

# Con check (raise exception se fallisce)
try:
    subprocess.run(["git", "status"], check=True)
except subprocess.CalledProcessError as e:
    print(f"Command failed with exit code {e.returncode}")

# Con input
result = subprocess.run(
    ["grep", "python"],
    input="python is great\njava is good\n",
    capture_output=True,
    text=True
)
print(result.stdout)  # python is great
\end{lstlisting}

\subsection{Automazione file system}

\begin{lstlisting}
import os
import shutil
from pathlib import Path

# Backup files
def backup_directory(source, dest):
    """Copia directory con backup"""
    if Path(dest).exists():
        # Backup esistente
        backup_path = f"{dest}.backup"
        shutil.move(dest, backup_path)

    shutil.copytree(source, dest)
    print(f"Backed up {source} to {dest}")

# Cleanup old files
def cleanup_old_files(directory, days=30):
    """Rimuovi file più vecchi di N giorni"""
    import time

    now = time.time()
    cutoff = now - (days * 86400)

    for file in Path(directory).glob("*"):
        if file.is_file() and file.stat().st_mtime < cutoff:
            print(f"Removing old file: {file}")
            file.unlink()

# Batch rename
def batch_rename(directory, pattern, replacement):
    """Rename files con pattern"""
    for file in Path(directory).glob(pattern):
        new_name = file.name.replace(pattern.replace("*", ""), replacement)
        file.rename(file.parent / new_name)
        print(f"Renamed: {file.name} → {new_name}")
\end{lstlisting}

\section{Web Scraping con requests}

\subsection{Installazione}

\begin{lstlisting}[language=bash]
pip install requests beautifulsoup4 lxml
\end{lstlisting}

\subsection{GET requests}

\begin{lstlisting}
import requests

# GET request semplice
response = requests.get("https://api.github.com/users/python")

# Check status
if response.status_code == 200:
    data = response.json()
    print(f"Name: {data['name']}")
    print(f"Repos: {data['public_repos']}")
else:
    print(f"Error: {response.status_code}")

# Con parameters
params = {"q": "python", "sort": "stars"}
response = requests.get("https://api.github.com/search/repositories", params=params)

# Con headers
headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get("https://example.com", headers=headers)

# Con timeout
try:
    response = requests.get("https://example.com", timeout=5)
except requests.Timeout:
    print("Request timed out")
\end{lstlisting}

\subsection{POST requests}

\begin{lstlisting}
# POST con JSON
data = {"username": "alice", "email": "alice@example.com"}
response = requests.post(
    "https://api.example.com/users",
    json=data,
    headers={"Content-Type": "application/json"}
)

# POST con form data
form_data = {"username": "alice", "password": "secret"}
response = requests.post("https://example.com/login", data=form_data)

# POST con file upload
files = {"file": open("document.pdf", "rb")}
response = requests.post("https://example.com/upload", files=files)
\end{lstlisting}

\subsection{Sessions e cookies}

\begin{lstlisting}
# Session mantiene cookies tra requests
session = requests.Session()

# Login
session.post("https://example.com/login", data={"user": "alice", "pass": "secret"})

# Richieste successive usano cookies automaticamente
response = session.get("https://example.com/dashboard")

# Session con headers default
session.headers.update({"User-Agent": "My Bot 1.0"})

response = session.get("https://example.com/api")
\end{lstlisting}

\section{BeautifulSoup: Parsing HTML}

\subsection{Basics}

\begin{lstlisting}
from bs4 import BeautifulSoup
import requests

url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "lxml")

# Find elementi
title = soup.find("title")
print(title.text)

# Find all
links = soup.find_all("a")
for link in links:
    print(link.get("href"))

# CSS selectors
articles = soup.select(".article")
first_paragraph = soup.select_one("p")

# Navigazione
div = soup.find("div", class_="container")
h2 = div.find("h2")
paragraph = h2.find_next_sibling("p")
\end{lstlisting}

\subsection{Esempio: Scraping news}

\begin{lstlisting}
import requests
from bs4 import BeautifulSoup

def scrape_news(url):
    """Scrape news articles"""
    response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.content, "lxml")

    articles = []

    for article in soup.select(".article"):
        title = article.select_one("h2").text.strip()
        link = article.select_one("a")["href"]
        summary = article.select_one(".summary").text.strip()

        articles.append({
            "title": title,
            "link": link,
            "summary": summary
        })

    return articles

# Uso
news = scrape_news("https://news.example.com")
for article in news:
    print(f"{article['title']}: {article['link']}")
\end{lstlisting}

\section{API REST}

\subsection{Consumo API GitHub}

\begin{lstlisting}
import requests

class GitHubAPI:
    BASE_URL = "https://api.github.com"

    def __init__(self, token=None):
        self.session = requests.Session()
        if token:
            self.session.headers["Authorization"] = f"token {token}"

    def get_user(self, username):
        response = self.session.get(f"{self.BASE_URL}/users/{username}")
        response.raise_for_status()
        return response.json()

    def get_repos(self, username):
        response = self.session.get(
            f"{self.BASE_URL}/users/{username}/repos",
            params={"sort": "updated", "per_page": 100}
        )
        response.raise_for_status()
        return response.json()

# Uso
api = GitHubAPI()
user = api.get_user("python")
print(f"Python organization has {user['public_repos']} public repos")

repos = api.get_repos("python")
for repo in repos[:5]:
    print(f"- {repo['name']}: {repo['stargazers_count']} stars")
\end{lstlisting}

\section{Selenium: Scraping dinamico}

\subsection{Setup}

\begin{lstlisting}[language=bash]
pip install selenium webdriver-manager

# Download ChromeDriver automaticamente con webdriver-manager
\end{lstlisting}

\subsection{Basics}

\begin{lstlisting}
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Setup driver
driver = webdriver.Chrome(ChromeDriverManager().install())

# Naviga
driver.get("https://example.com")

# Trova elementi
title = driver.find_element(By.TAG_NAME, "h1")
print(title.text)

# Click
button = driver.find_element(By.ID, "submit-button")
button.click()

# Input text
input_field = driver.find_element(By.NAME, "username")
input_field.send_keys("alice")

# Wait per elemento
element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, "result"))
)

# Screenshot
driver.save_screenshot("screenshot.png")

# Close
driver.quit()
\end{lstlisting}

\section{Etica e Best Practices}

\subsection{robots.txt}

\begin{lstlisting}
import requests
from urllib.robotparser import RobotFileParser

def can_fetch(url, user_agent="*"):
    """Check se posso scrapare URL"""
    robots_url = f"{url.rstrip('/')}/robots.txt"

    rp = RobotFileParser()
    rp.set_url(robots_url)
    rp.read()

    return rp.can_fetch(user_agent, url)

# Uso
if can_fetch("https://example.com/page"):
    # OK to scrape
    response = requests.get("https://example.com/page")
else:
    print("Not allowed by robots.txt")
\end{lstlisting}

\subsection{Rate Limiting}

\begin{lstlisting}
import time
import requests

class RateLimitedScraper:
    def __init__(self, delay=1.0):
        self.delay = delay
        self.last_request = 0

    def get(self, url):
        # Wait se troppo veloce
        elapsed = time.time() - self.last_request
        if elapsed < self.delay:
            time.sleep(self.delay - elapsed)

        self.last_request = time.time()
        return requests.get(url)

# Uso: max 1 request/second
scraper = RateLimitedScraper(delay=1.0)

for url in urls:
    response = scraper.get(url)
    # Process...
\end{lstlisting}

\subsection{Retry con backoff}

\begin{lstlisting}
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def requests_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504),
):
    session = requests.Session()

    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )

    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session

# Uso
session = requests_retry_session()
response = session.get("https://example.com")
# Automatically retries on connection errors
\end{lstlisting}

\section{Salvataggio Dati}

\subsection{CSV}

\begin{lstlisting}
import csv

# Scrape e salva CSV
articles = scrape_news("https://news.example.com")

with open("news.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["title", "link", "summary"])
    writer.writeheader()
    writer.writerows(articles)
\end{lstlisting}

\subsection{JSON}

\begin{lstlisting}
import json

# Salva JSON
with open("news.json", "w", encoding="utf-8") as f:
    json.dump(articles, f, indent=2, ensure_ascii=False)

# Append JSON (JSONLines format)
with open("news.jsonl", "a", encoding="utf-8") as f:
    for article in articles:
        f.write(json.dumps(article, ensure_ascii=False) + "\n")
\end{lstlisting}

\subsection{Database}

\begin{lstlisting}
import sqlite3

# Save to SQLite
conn = sqlite3.connect("news.db")
cursor = conn.cursor()

cursor.execute("""
    CREATE TABLE IF NOT EXISTS articles (
        id INTEGER PRIMARY KEY,
        title TEXT,
        link TEXT UNIQUE,
        summary TEXT,
        scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
""")

for article in articles:
    cursor.execute(
        "INSERT OR IGNORE INTO articles (title, link, summary) VALUES (?, ?, ?)",
        (article["title"], article["link"], article["summary"])
    )

conn.commit()
conn.close()
\end{lstlisting}

\section{Scheduling}

\subsection{Con schedule}

\begin{lstlisting}[language=bash]
pip install schedule
\end{lstlisting}

\begin{lstlisting}
import schedule
import time

def job():
    print("Running scraping job...")
    articles = scrape_news("https://news.example.com")
    save_to_database(articles)
    print(f"Scraped {len(articles)} articles")

# Schedule
schedule.every().hour.do(job)                 # Ogni ora
schedule.every().day.at("10:30").do(job)      # Ogni giorno alle 10:30
schedule.every().monday.at("09:00").do(job)   # Ogni lunedì alle 9:00

# Run
while True:
    schedule.run_pending()
    time.sleep(60)  # Check every minute
\end{lstlisting}

\subsection{Con cron (Linux/Mac)}

\begin{lstlisting}[language=bash]
# Modifica crontab
crontab -e

# Run script ogni ora
0 * * * * /usr/bin/python3 /path/to/scraper.py

# Run ogni giorno alle 2 AM
0 2 * * * /usr/bin/python3 /path/to/scraper.py

# Run ogni lunedì alle 9 AM
0 9 * * 1 /usr/bin/python3 /path/to/scraper.py

# Formato: minute hour day month weekday command
\end{lstlisting}

\section{Progetto Completo: News Aggregator}

\begin{lstlisting}
import requests
from bs4 import BeautifulSoup
import sqlite3
import schedule
import time
from datetime import datetime

class NewsAggregator:
    def __init__(self, db_path="news.db"):
        self.db_path = db_path
        self.setup_database()

    def setup_database(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY,
                source TEXT,
                title TEXT,
                link TEXT UNIQUE,
                summary TEXT,
                scraped_at TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()

    def scrape_source(self, name, url, selector):
        """Scrape single source"""
        try:
            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
            soup = BeautifulSoup(response.content, "lxml")

            articles = []
            for article in soup.select(selector):
                title = article.select_one("h2").text.strip()
                link = article.select_one("a")["href"]
                summary = article.select_one(".summary").text.strip()

                articles.append({
                    "source": name,
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "scraped_at": datetime.now()
                })

            return articles

        except Exception as e:
            print(f"Error scraping {name}: {e}")
            return []

    def save_articles(self, articles):
        """Save to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        saved = 0
        for article in articles:
            try:
                cursor.execute(
                    "INSERT INTO articles (source, title, link, summary, scraped_at) VALUES (?, ?, ?, ?, ?)",
                    (article["source"], article["title"], article["link"], article["summary"], article["scraped_at"])
                )
                saved += 1
            except sqlite3.IntegrityError:
                pass  # Duplicate

        conn.commit()
        conn.close()
        return saved

    def run(self):
        """Run scraping job"""
        sources = [
            ("TechNews", "https://technews.example.com", ".article"),
            ("ScienceDaily", "https://sciencedaily.example.com", ".news-item"),
        ]

        all_articles = []
        for name, url, selector in sources:
            print(f"Scraping {name}...")
            articles = self.scrape_source(name, url, selector)
            all_articles.extend(articles)
            time.sleep(1)  # Rate limiting

        saved = self.save_articles(all_articles)
        print(f"Saved {saved} new articles out of {len(all_articles)}")

# Usage
aggregator = NewsAggregator()

# Run once
aggregator.run()

# Schedule
schedule.every().hour.do(aggregator.run)

while True:
    schedule.run_pending()
    time.sleep(60)
\end{lstlisting}

\begin{tcolorbox}[title=Best Practices]
Il rispetto di robots.txt e Terms of Service è un requisito etico e legale fondamentale: ignorare queste policy può portare a ban dell'IP o azioni legali. Verificare sempre cosa è permesso scrapare prima di iniziare.

L'uso di un User-Agent header descrittivo che identifichi il proprio scraper aiuta a evitare ban automatici e permette ai webmaster di contattarti se necessario. Header generici o assenti sono spesso bloccati.

Implementare rate limiting è essenziale per non sovraccaricare i server target. Un limite di 1-2 richieste al secondo è generalmente ragionevole, ma dovrebbe essere ridotto per siti più piccoli o aumentato solo se esplicitamente permesso.

La strategia di retry con exponential backoff gestisce errori temporanei di rete in modo intelligente, aumentando progressivamente il tempo di attesa tra tentativi per non aggravare problemi di carico del server.

Gestire timeout appropriati (tipicamente 10-30 secondi) previene che lo scraper rimanga bloccato indefinitamente su richieste lente o server non responsivi.

Salvare dati incrementalmente anziché accumulare tutto in memoria e scrivere alla fine previene la perdita totale dei dati in caso di crash, permettendo di riprendere da dove ci si è fermati.

Il logging degli errori è fondamentale per debugging e monitoraggio, permettendo di identificare pattern di fallimento e problemi ricorrenti con specifici siti o selectors.

L'uso di selectors CSS robusti basati su attributi stabili (id, data attributes) piuttosto che classi generate dinamicamente riduce la fragilità dello scraper rispetto a cambiamenti del sito.

Validare i dati estratti prima di salvarli garantisce che solo dati corretti e completi vengano persistiti, evitando corruzioni del dataset.

Infine, evitare di scrapare durante picchi di traffico (ad esempio ore di punta) riduce l'impatto sul server target e diminuisce le probabilità di essere rilevati come bot malevoli.
\end{tcolorbox}

\begin{tcolorbox}[title=Errori Comuni]
Ignorare robots.txt è uno degli errori più gravi, comportando rischio di ban immediato e potenziali conseguenze legali. Verificare sempre le policy prima di scrapare.

Inviare troppi requests troppo velocemente è una pratica che porta rapidamente al ban dell'IP. I server rilevano facilmente pattern di traffico non umani e implementano protezioni automatiche.

L'uso di selectors fragili basati su classi CSS generate automaticamente (come "btn-primary-2024-v3") rende lo scraper estremamente vulnerabile a rotture quando il sito viene aggiornato. Preferire selectors stabili basati su semantica o data attributes.

Non gestire gli errori di rete porta a crash dello scraper su problemi temporanei facilmente risolvibili con retry. Ogni richiesta HTTP può fallire e deve essere gestita appropriatamente.

Non usare timeout causa script che rimangono bloccati indefinitamente su connessioni lente o server non responsivi, consumando risorse inutilmente.

Hardcodare selectors CSS nel codice invece di configurarli esternamente rende necessario ricompilare ogni volta che i selectors cambiano, aumentando la manutenzione.

Non validare i dati estratti porta ad accumulare dati incompleti o corrotti nel dataset, che devono poi essere puliti manualmente in post-processing.

Non usare sessions HTTP fa perdere cookies e stato di autenticazione tra richieste, rendendo impossibile scrapare siti che richiedono login o mantengono stato di sessione.

Scrapare dati personali senza consenso esplicito viola il GDPR e altre normative sulla privacy, con conseguenze legali potenzialmente gravi.

Infine, non gestire correttamente l'encoding dei caratteri porta a dati corrotti con caratteri speciali renderizzati incorrettamente, specialmente per lingue non inglesi.
\end{tcolorbox}

\section{Esercizi}

\subsection{Livello Base}
\begin{enumerate}
  \item Scrivi script che scarica homepage di 5 siti e salva HTML
  \item Scrape titoli articoli da news website e salva CSV
  \item Automatizza backup directory con subprocess
\end{enumerate}

\subsection{Livello Intermedio}
\begin{enumerate}
  \item Implementa rate-limited scraper con retry logic
  \item Consuma GitHub API e salva repo più starred in database
  \item Schedule script che scrape ogni ora con schedule
  \item Parse HTML table e converti in CSV
\end{enumerate}

\subsection{Livello Avanzato}
\begin{enumerate}
  \item Implementa news aggregator completo (3+ sources, DB, scheduling)
  \item Scrape sito con login usando Selenium
  \item Crea API wrapper per servizio esterno con caching
  \item Implementa distributed scraping con multiprocessing
\end{enumerate}

\section{Riferimenti}
\begin{itemize}
  \item requests: \url{https://requests.readthedocs.io/}
  \item BeautifulSoup: \url{https://www.crummy.com/software/BeautifulSoup/bs4/doc/}
  \item Selenium: \url{https://selenium-python.readthedocs.io/}
  \item schedule: \url{https://schedule.readthedocs.io/}
\end{itemize}
